## Distributed Training
### Lightning Fabric
![[Pasted image 20230619093411.png]]
ref: [Sebastian Raschka on Twitter: "@bbk005 @DrJimFan @karpathy None of these: just PyTorch with distributed training and tensor sharding. Optionally with CPU offloading for really big LLMs. I use Fabric as a convenient wrapper here." / Twitter](https://twitter.com/rasbt/status/1670130547669041154)

>[!info]
>- Demo: [rasbt/pytorch-fabric-demo (github.com)](https://github.com/rasbt/pytorch-fabric-demo/tree/main)
>- Framework: [Welcome to ⚡ Fabric — lightning 2.0.3 documentation](https://lightning.ai/docs/fabric/stable/)

